version: '3.8'

services:
  conduktor-postgresql:
    image: postgres:latest
    container_name: conduktor-postgresql
    volumes:
      - conduktor_pg_data:/var/lib/postgresql/data
    environment:
      POSTGRES_DB: "conduktor-console"
      POSTGRES_USER: "conduktor"
      POSTGRES_PASSWORD: "change_me"
      POSTGRES_HOST_AUTH_METHOD: "scram-sha-256"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - spark

  conduktor-console:
    image: conduktor/conduktor-console:1.34.1
    container_name: conduktor-console
    ports:
      - "8080:8080"
    volumes:
      - conduktor_data:/var/conduktor
    environment:
      CDK_ORGANIZATION_NAME: "getting-started"
      CDK_DATABASE_URL: "postgresql://conduktor:change_me@conduktor-postgresql:5432/conduktor-console"
      CDK_MONITORING_CORTEX-URL: http://conduktor-monitoring:9009/
      CDK_MONITORING_ALERT-MANAGER-URL: http://conduktor-monitoring:9010/
      CDK_MONITORING_CALLBACK-URL: http://conduktor-platform:8080/monitoring/api/
      CDK_MONITORING_NOTIFICATIONS-CALLBACK-URL: http://localhost:8080
    depends_on:
      conduktor-postgresql:
        condition: service_healthy
      kafka1:
        condition: service_started
    networks:
      - spark
      - kafka

  conduktor-monitoring:
    image: conduktor/conduktor-console-cortex:1.34.1
    container_name: conduktor-monitoring
    environment:
      CDK_CONSOLE-URL: "http://conduktor-console:8080"
    networks:
      - spark

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - 9870:9870
      - 9010:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    env_file:
      - ./docker-hadoop-spark-master/hadoop.env
    networks:
      - spark

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
    ports:
      - "9864:9864"
    env_file:
      - ./docker-hadoop-spark-master/hadoop.env
    networks:
      - spark

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864"
    env_file:
      - ./docker-hadoop-spark-master/hadoop.env
    networks:
      - spark

  nodemanager:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    env_file:
      - ./docker-hadoop-spark-master/hadoop.env
    networks:
      - spark

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: historyserver
    restart: always
    environment:
      SERVICE_PRECONDITION: "namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088"
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    env_file:
      - ./docker-hadoop-spark-master/hadoop.env
    networks:
      - spark

  spark-master:
    image: bde2020/spark-master:3.2.1-hadoop3.2
    container_name: spark-master
    depends_on:
      - namenode
      - datanode
    volumes:
      - ./docker-hadoop-spark-master/custom_config/hive-site.xml:/spark/conf/hive-site.xml
    ports:
      - "8081:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks:
      - spark

  spark-worker-1:
    image: bde2020/spark-worker:3.2.1-hadoop3.2
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8082:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    networks:
      - spark

  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    depends_on:
      - namenode
      - datanode
    env_file:
      - ./docker-hadoop-spark-master/hadoop-hive.env
    environment:
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore/metastore"
      SERVICE_PRECONDITION: "hive-metastore:9083"
    ports:
      - "10000:10000"
    networks:
      - spark

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    env_file:
      - ./docker-hadoop-spark-master/hadoop-hive.env
    command: /opt/hive/bin/hive --service metastore
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode:9864 hive-metastore-postgresql:5432"
    ports:
      - "9083:9083"
    networks:
      - spark

  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.3.0
    container_name: hive-metastore-postgresql
    ports:
      - "5433:5432"
    networks:
      - spark

  presto-coordinator:
    image: shawnzhu/prestodb:0.181
    container_name: presto-coordinator
    ports:
      - "8089:8089"
    networks:
      - spark

  generate-db-data:
    build: ./init-db-generator
    image: generate-db-data
    networks:
      - postgres

  generate-kafka-data:
    build: ./kafka-generator
    image: generate-kafka-data
    depends_on:
      - kafka1
    networks:
      - kafka
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka1:19092

  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - kafka

  kafka1:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka1
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "19092:19092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka1:29092,PLAINTEXT_HOST://localhost:19092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
    networks:
      - kafka

  kafka-to-hdfs:
    build: ./kafka-to-hdfs-spark
    container_name: kafka-to-hdfs
    depends_on:
      - spark-master
      - kafka1
      - namenode
    networks:
      - spark
      - kafka
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - KAFKA_BOOTSTRAP_SERVERS=kafka1:29092
      - HDFS_NAMENODE=namenode:9000

  jupyterlab:
    build: ./jupiter
    container_name: jupyter
    environment:
      - GRANT_SUDO=yes
      - JUPYTER_ENABLE_LAB=yes
    user: root
    ports:
      - "8888:8888"
    networks:
      - jupyter

  postgres:
    image: postgres:latest
    container_name: postgres_container
    environment:
      POSTGRES_USER: postgres_user
      POSTGRES_PASSWORD: postgres_password
      POSTGRES_DB: postgres_db
      PGDATA: /var/lib/postgresql/data/pgdata
    ports:
      - "5430:5432"
    volumes:
      - ./postgres/pgdata:/var/lib/postgresql/data/pgdata
      - ./postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U postgres_user -d postgres_db" ]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped
    tty: true
    stdin_open: true
    networks:
      - postgres

  postgres-to-hdfs:
    build: ./init-db-generator
    container_name: postgres-to-hdfs
    depends_on:
      - postgres
      - namenode
      - spark-master
    environment:
      - JDBC_JAR_PATH=init-db-generator/postgresql-42.7.7.jar
      - JDBC_URL=jdbc:postgresql://postgres:5432/postgres_db
      - JDBC_USER=postgres_user
      - JDBC_PASSWORD=postgres_password
      - JDBC_DRIVER=org.postgresql.Driver
      - HDFS_BASE_PATH=hdfs://namenode:9000/user/hadoop/
      - EXPORT_TABLES=data.cards,data.banks,data.peoples,data.organizations,data.mcc
    networks:
      - spark
      - postgres
    command: ["python", "export_to_hdfs.py"]

networks:
  spark:
    external: true
    name: disable_ipv6
  postgres:
    external: true
    name: disable_ipv6
  kafka:
    external: true
    name: disable_ipv6
  jupyter:
    external: true
    name: disable_ipv6

volumes:
  conduktor_pg_data:
  conduktor_data:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_historyserver:
  pgdata:
    driver: local 